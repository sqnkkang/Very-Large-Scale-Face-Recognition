# 模型架构

## 1 >> 模型概述

## 2 >> mobilefacenet 网络

#### 2.1 >> 参数信息

根据参数信息，构建我们的网络形式，应用在构建瓶颈残差块的时候，默认为下面的形式，可以修改，但要注意前后的输入输出通道数要相符。

```python
MobileFaceNet_BottleNeck_Setting = [
    # expansion, oup, for ,stride
    [2, 64, 5, 2],
    [4, 128, 1, 2],
    [2, 128, 6, 1],
    [4, 128, 1, 2],
    [2, 128, 2, 1]
]
````

#### 2.2 >> BottleNeck类，瓶颈残差块

`__init__` 初始化函数，传进来的参数包含输入通道、输出通道、步长、拓展因子。残差条件为步长为 1，并且输入通道数和输出的通道数相同（也只有这样才能够直接相加），构建网络的时候主要包含 1 x 1 卷积拓展通道，3 x 3 卷积提取特征，然后再 1 x 1 卷积压缩通道，也就是降维。类实现的是给定输入的通道鼠标和输出的通道数计算，进行卷积输出对应的结果，参数信息也是为了传参数到该类里面多次堆叠层数来定义的，然后前向传播即可。

```python
class BottleNeck(nn.Module):
    def __init__(self, inp, oup, stride, expansion):
        super(BottleNeck, self).__init__()
        self.connect = stride == 1 and inp == oup

        self.conv = nn.Sequential(

            nn.Conv2d(inp, inp * expansion, 1, 1, 0, bias=False),
            nn.BatchNorm2d(inp * expansion),
            nn.PReLU(inp * expansion),

            nn.Conv2d(inp * expansion, inp * expansion, 3, stride, 1, groups=inp * expansion, bias=False),
            nn.BatchNorm2d(inp * expansion),
            nn.PReLU(inp * expansion),

            nn.Conv2d(inp * expansion, oup, 1, 1, 0, bias=False),
            nn.BatchNorm2d(oup),
        )

    def forward(self, x):
        if self.connect:
            return x + self.conv(x)
        else:
            return self.conv(x)
```

#### 2.3 ConvBlock类，卷积模块

`__init__` 初始化函数，传进来的参数包含，输入的通道数，输出的通道数，卷积核的大小，步长和边缘的 `padding` 还有是否是 `dw` 卷积和是否线性输出，不进行激活。按照传进来的参数进行反向传播即可。

```python
class ConvBlock(nn.Module):
    def __init__(self, inp, oup, k, s, p, dw=False, linear=False):
        super(ConvBlock, self).__init__()
        self.linear = linear
        if dw:
            self.conv = nn.Conv2d(inp, oup, k, s, p, groups=inp, bias=False)
        else:
            self.conv = nn.Conv2d(inp, oup, k, s, p, bias=False)

        self.bn = nn.BatchNorm2d(oup)
        if not linear:
            self.prelu = nn.PReLU(oup)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        if self.linear:
            return x
        else:
            return self.prelu(x)
```

#### 2.2 >> 网络类，MobileFaceNet

`__init__` 初始化函数，传进来三个参数，分别是特征维度，是否启用混合精度训练，还有就是网络的参数信息，`super(MobileFaceNet, self).__init__()` 这句话的含义是从 `nn.Moudle` 里面继承他的初始化，`PyTorch` 中所有自定义网络必须继承 `nn.Module`，因为它提供了网络层的注册机制（如 `self.conv1` 会被自动识别为子模块）,支持参数管理（如 model.parameters() 获取所有可训练参数），十分的方便。

其中这句的用法比较特殊 `block = BottleNeck` 这是将实例直接赋值的意思，下面调用 `block()` 的话相当于直接调用 `BottleNeck` 的初始化方法。

```python
class MobileFaceNet(nn.Module):
    def __init__(self, feat_dim=128, fp16=False, bottleneck_setting=MobileFaceNet_BottleNeck_Setting):
        super(MobileFaceNet, self).__init__()
        self.conv1 = ConvBlock(3, 64, 3, 2, 1)
        self.dw_conv1 = ConvBlock(64, 64, 3, 1, 1, dw=True)

        self.cur_channel = 64
        block = BottleNeck
        self.blocks = self._make_layer(block, bottleneck_setting)

        self.conv2 = ConvBlock(128, 512, 1, 1, 0)
        self.linear7 = ConvBlock(512, 512, 7, 1, 0, dw=True, linear=True)
        self.linear1 = ConvBlock(512, feat_dim, 1, 1, 0, linear=True)
        self.fp16 = fp16
    ...
```

`_make_layer` 函数，参数为 `block` 类的实例，和参数信息，网络的构建（堆叠）按照参数信息来实现。`nn.Sequential(*layers)` 等价于 `nn.Sequential(layer1, layer2, layer3, ...)` 将列表 `layers` 解包为独立参数传递给 `Sequential`。

```python
    ...
    def _make_layer(self, block, setting):
        layers = []
        for t, c, n, s in setting:
            for i in range(n):
                if i == 0:
                    layers.append(block(self.cur_channel, c, s, t))
                else:
                    layers.append(block(self.cur_channel, c, 1, t))
                self.cur_channel = c

        return nn.Sequential(*layers)
    ...
```

`forward` 函数，一个参数，传进来原始图像，前向传播结束，返回回去，分为混合精度和直接计算两种，前者会节省显存的空间。

```python
    ...
    def forward(self, x):
        if self.fp16:
            with torch.cuda.amp.autocast():    
                x = self.conv1(x)
                x = self.dw_conv1(x)
                x = self.blocks(x)
                x = self.conv2(x)
                x = self.linear7(x)
                x = self.linear1(x)
                x = torch.flatten(x, 1)
                return F.normalize(x)
        else:
            x = self.conv1(x)
            x = self.dw_conv1(x)
            x = self.blocks(x)
            x = self.conv2(x)
            x = self.linear7(x)
            x = self.linear1(x)
            x = torch.flatten(x, 1)
            return F.normalize(x)
```

## 3 >> resnet_arcface 网络

## 4 >> resnet_std 网络

## 5 >> 构建步骤

你现在已经完成了模型的构建，检查没有问题的话，还剩下：

- 003 >> [数据加载器](https://github.com/sqnkkang/Very-Large-Scale-Face-Recognition/blob/master/recognition/003.md)
- 004 >> [LRU策略](https://github.com/sqnkkang/Very-Large-Scale-Face-Recognition/blob/master/recognition/004.md)
- 005 >> [FFC-DCP动态池](https://github.com/sqnkkang/Very-Large-Scale-Face-Recognition/blob/master/recognition/005.md)
- 006 >> [训练](https://github.com/sqnkkang/Very-Large-Scale-Face-Recognition/blob/master/recognition/006.md)
- 007 >> [测试](https://github.com/sqnkkang/Very-Large-Scale-Face-Recognition/blob/master/recognition/007.md)

## 6 >> 致谢

本文受 [Build-Your-Own-Face-Model](https://github.com/siriusdemon/Build-Your-Own-Face-Model/) 与 [FFC](https://github.com/tiandunx/FFC/) 的启发，主要用作作者学习使用。
